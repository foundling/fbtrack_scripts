#!/usr/bin/env python3

'''
    Aggregate JSON FitBit data for all subjects in ACT study.
    Write to {sleep, daily and hourly} output files.
'''

import argparse
import csv
from datetime import datetime, timedelta
from itertools import groupby
import json
import os
import re
from shutil import rmtree
import sys

from lib.headers import headers
from lib.daily import build_daily
from lib.hourly import build_hourly
from lib.sleep import build_sleep
from lib.sleep_by_minute import build_sleep_minute

parser = argparse.ArgumentParser(description='fb_aggregate script')
parser.add_argument('source', type=str, help='directory location for source JSON files')
parser.add_argument('dest', type=str, help='output directory for aggregated CSV data')
source_directory_arg = parser.parse_args().source
output_directory_arg = parser.parse_args().dest

SCRIPT_ROOT = os.path.abspath(os.curdir) 
SRC_DIR = os.path.realpath(source_directory_arg) 
DEST_DIR = os.path.realpath(output_directory_arg)
NOW = datetime.now()
FILE_TIMESTAMP = NOW.strftime('%Y-%m-%d')
DATA_DIR_TIMESTAMP = NOW.strftime('%Y-%m-%d_%H.%M.%S')
CONVERTED_DATA_DIR = os.path.abspath(os.path.join(DEST_DIR, DATA_DIR_TIMESTAMP)) 


def validate_args():

    if not os.path.isdir(source_directory_arg):
        print('Source directory does not exist or is not a directory: ', source_directory_arg) 
        sys.exit(1)

    if not os.path.isdir(output_directory_arg):
        print('Dest directory does not exist or is not a directory: ', output_directory_arg) 
        sys.exit(1)


def get_subject_id(filename):
    return os.path.basename(filename).split('_', maxsplit=1)[0]


def get_collection_date(filename):
    return os.path.basename(filename).split('_', maxsplit=2)[1]


def combine_daily_subject_metric_files(filepaths_by_subject_id):

    '''
    For each subject, gather their metric data for each day into a single dictionary and write that out
    as .json into a '<subject id>_<date string>_all.json' file. 
    '''

    for subject_id, json_paths in filepaths_by_subject_id:

        subject_metrics_by_date = list(
            (datestring, list(metrics))
            for datestring, metrics 
            in groupby(json_paths, get_collection_date)
        )


        for datestring, metric_filenames in subject_metrics_by_date:

            aggregated_metrics_for_day = {} 
            aggregated_fname = '{}_{}_all.json'.format(subject_id, datestring)
            aggregated_output_filepath = os.path.abspath(os.path.join(SRC_DIR, aggregated_fname))

            for filename in metric_filenames:

                metric = os.path.basename(filename).split('_')[2].split('.')[0].replace('activities-','') # take string between last '_' and '.json', replace 'activities-' with ''
                filepath = os.path.abspath(os.path.join(SRC_DIR, filename)) 

                with open(filepath) as json_file:
                    json_obj = json.load(json_file)
                    for key in json_obj.keys(): 
                        if (key == 'summary'):
                            aggregated_metrics_for_day['{}-summary'.format(metric)] = json_obj[key] # summary could be overwritten by activities summary
                        else:
                            aggregated_metrics_for_day[key] = json_obj[key] # summary could be overwritten by activities summary
            print(aggregated_metrics_for_day.keys())

            with open(aggregated_output_filepath, 'w') as f:
                json.dump(aggregated_metrics_for_day, f, sort_keys=True, indent=4)


def setup():

    ''' 
    - creates the time-stamped csv output directory for a given run of this script, e.g. 20-09-05/ 
    - for each subject, combines metrics for a given day and writes to a time-stamped 'all.json' file.
    '''

    # create directory for daily, hourly, etc CSV files converted from Fitbit's json format
    os.mkdir(CONVERTED_DATA_DIR)

    # filter all json data files, excluding any existing 'all' files 
    json_filepaths = [  os.path.join(SRC_DIR, filename) 
                        for filename 
                        in os.listdir(SRC_DIR) 
                        if filename.endswith('.json') and not filename.endswith('all.json') ]

    # group them into lists of files by subject id
    filepaths_by_subject_id = list(
        (subject_id, list(files))
        for subject_id, files
        in groupby(json_filepaths, get_subject_id)
    )

    combine_daily_subject_metric_files(filepaths_by_subject_id)


def main():


    setup()

    '''
    takes the all.json files and process them using the modules imported from lib/
    and finally writes them out to a time-stamped directory as '<metric>.csv'.
    '''

    all_metric_files = [ 
        os.path.join(SRC_DIR, filename)
        for filename 
        in os.listdir(SRC_DIR) 
        if filename.endswith('_all.json') 
    ]

    aggregators = {
        'sleep':  build_sleep,
        'daily':  build_daily,
        'hourly': build_hourly,
        'sleep_by_minute': build_sleep_minute
    }

    for aggregation_type in aggregators.keys():

        metric_output_filename = '{}.csv'.format(aggregation_type)
        outfile_path = os.path.join(CONVERTED_DATA_DIR, metric_output_filename)

        with open(outfile_path, 'a+') as outfile:

            csv_writer = csv.writer(outfile)
            csv_writer.writerow(headers[ aggregation_type ])

            for filepath in all_metric_files:

                subject_id = os.path.basename(filepath).split('_')[0]
                dataset = json.loads( open(filepath).read() )
                dataset_datestring = os.path.basename(filepath).split('_')[1]
                dataset_date = datetime.strptime(dataset_datestring, '%Y-%m-%d')

                aggregator = aggregators[ aggregation_type ]
                aggregated_data = aggregator(subject_id, dataset)

                # the hourly aggregator returns a row of rows, so use csv's .writerows method 
                if (aggregation_type != 'daily'): 
                    csv_writer.writerows(aggregated_data)
                else:
                    csv_writer.writerow(aggregated_data)

if __name__ == '__main__':

    validate_args()
    main()
